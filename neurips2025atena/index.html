<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Active Test-time Vision-Language Navigation">
  <meta name="keywords" content="ATENA">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ATENA</title>

  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>
  <style>  
    hr{  
        height: 4px;  
        background-color: red;  
        border: none;  
    }  
  </style> 

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Helvetica Neue|Castoro"
        rel="stylesheet">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/robot.png">
  <!-- <link rel="icon" href="./static/images/favicon.svg"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-widescreen">
      <div class="columns is-centered">
      <div class="columns is-full-width">
        <div class="column has-text-centered">
          <!-- <h1 class="title is-1 publication-title" style="font-size:400%;"><img src="static/images/vox1.png" alt="Image description" style="height: 70px; margin-bottom: 20px; margin-right: 20px; vertical-align: middle;">ProtoOcc <img src="static/images/vox2.png" alt="Image description" style="height: 70px; margin-bottom: 20px; margin-right: 20px; vertical-align: middle;"></h1> -->
          <!-- <h1 class="title is-1 publication-title" style="font-size:100%;">IEEE/CVF Conferecnce on Computer Vision and Pattern Recognition 2025</h1>
          <hr> -->
          <h1 class="title is-2 publication-title">Active Test-time Vision-Language Navigation</h1> 
          <div class="is-size-4 publication-authors" style="margin: 10px;"><span class="author-block"><b>NeurIPS 2025</b></span></div>
          <!-- <h1 class="title is-2 publication-title"><span style="color:#c204ac; font-weight:bold">C</span>ontext-<span style="color:#c204ac">A</span>ware <span style="color:#c204ac">T</span>ransformer with <span style="color:#c204ac">S</span>patial Guidance for Generalizable 3D Gaussian Splatting from A Single-View Image </h1> -->
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://scholar.google.com/citations?hl=ko&user=b5G0Ks0AAAAJ">Heeju Ko</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://sungjune-kim.github.io/">Sung June Kim</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://okr9871.github.io/">Gyeongrok Oh</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="">Jeongyoon Yoon</a><sup>1</sup>,</span><br>
            <span class="author-block">
              <a href="https://web.eecs.umich.edu/~honglak/">Honglak Lee</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://sujinjang.github.io/">Sujin Jang</a></a><sup>3</sup>
            <span class="author-block">
              <a href="https://cvlab.kaist.ac.kr/">Seungryong Kim</a><sup>4</sup>,</span>
            <span class="author-block">
              <a href="https://kuaicv.com/">Sangpil Kim</a></a><sup>1,*</sup>
          </div>

          <div class="is-size-5 publication-authors">
            <!-- <span class="author-block"><sup>1</sup><span style="color: #900C3F;">Korea University</span></span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -->
            <span class="author-block"><sup>1</sup>Korea University</span></span>&nbsp;&nbsp;&nbsp;&nbsp;
            <span class="author-block"><sup>2</sup>University of Michigan</span>&nbsp;&nbsp;&nbsp;&nbsp;
            <span class="author-block"><sup>3</sup>Samsung AI Center, DS Division</span>&nbsp;&nbsp;&nbsp;&nbsp;
            <span class="author-block"><sup>4</sup> KAIST AI</span></b></span>
          </div>
          * Corresponding Author

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2506.06630"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/kuai-lab/NeurIPS25_att_vln"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="section">  
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <div class="content has-text-justified">
          <div class="video-item">
            <video autoplay muted loop playsinline style="width:100%; border-radius:10px;">
              <source src="./static/videos/atena demo2.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
        </div>
        </div>
      </div>
    </div>
  </div>
</section>

<hr>


<!-- <section class="hero is-light is-small"> -->
<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <!-- <br> -->
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <!-- <p class="content is-size-6 has-text-left"> -->
          <div style="border: 3px dotted black; padding: 10px; margin-bottom: 20px; background-color: rgba(241, 243, 246, 0.831);">
          <h2 class="title is-3" style="text-align: center; font-size: 130%; margin-bottom: 0;">
            <span style="color: rgb(78, 80, 214);">TL;DR:</span> We present ATENA, a test-time adaptation framework for vision-language navigation to improve navigation performance under distribution shift.
          </h2>
          </div>
          <p>
            Vision-Language Navigation (VLN) policies trained on offline datasets often 
            exhibit degraded task performance when deployed in unfamiliar navigation environments 
            at test time, where agents are typically evaluated without access to external 
            interaction or feedback. Entropy minimization has emerged as a practical solution 
            for reducing prediction uncertainty at test time; however, it can suffer from accumulated 
            errors, as agents may become overconfident in incorrect actions without sufficient contextual 
            grounding. To tackle these challenges, we introduce ATENA (Active TEst-time Navigation Agent), 
            a test-time active learning framework that enables a practical human-robot interaction via 
            episodic feedback on uncertain navigation outcomes. In particular, ATENA learns to increase 
            certainty in successful episodes and decrease it in failed ones, improving uncertainty calibration. 
            Here, we propose <i>mixture entropy optimization</i>, where entropy is obtained from a combination of 
            the action and pseudo-expert distributions―a hypothetical action distribution assuming the 
            agent's selected action to be optimal―controlling both prediction confidence and action preference. 
            In addition, we propose a <i>self-active learning</i> strategy that enables an agent to evaluate 
            its navigation outcomes based on confident predictions. As a result, the agent stays actively 
            engaged throughout all iterations, leading to well-grounded and adaptive decision-making. 
            Extensive evaluations on challenging VLN benchmarks―REVERIE, R2R, and R2R-CE―demonstrate 
            that ATENA successfully overcomes distributional shifts at test time, outperforming the 
            compared baseline methods across various settings.
          </p>
          <!-- <br> -->
        </div>
      </div>
    </div>
  </div>
</section>


<hr>

<section class="section">
  <!-- <section class="hero is-light is-small"> -->
  <div class="container is-max-desktop">
    <!-- Paper Pipeline. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">How does ATENA work?</h2>
        <img src="./static/images/main_figure.png" alt="Paper pipeline." style="width: 100%; padding-top: 10px"/>
        <div class="content has-text-justified">
          <p style="padding-top: 10px">
            <b>Overview of the ATENA adaptation framework. </b> At each navigation step, the agent
            stores state and entropy information in its memory. Once the episode ends, the stored entropy is used
            to determine the feedback source: human oracle for uncertain episodes, and self oracle for certain
            episodes. Self oracle utilizes a self-prediction head, trained during online test-time, enabling the
            agent to autonomously predict navigation success or failure by itself.
          </p>
          <br>
        </div>
        <img src="./static/images/atena_meo.png" alt="Paper pipeline." style="width: 100%; padding-top: 10px"/>
        <div class="content has-text-justified">
          <p style="padding-top: 10px">
            <b>Mixture Entropy Optimization (MEO). </b> 
            (a) We design the <i>Mixture Action Distribution</i> by combining the model’s action distribution (yellow)
            with a pseudo-expert distribution (red). (b) MEO minimizes mixture entropy for successful episodes—reinforcing correct actions—and maximizes it for failed ones, discouraging incorrect behaviors.
          </p>
          <!-- <br> -->
        </div>
      </div>
    </div>
  </div>
</section>

<hr>

<section class="section">
<!-- <section class="hero is-light is-small"> -->
  <!-- <section class="hero is-light is-small"> -->
  <div class="container is-max-desktop">
    <!-- Paper Pipeline. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <!-- <br> -->
        <h2 class="title is-3">Quantitative Results</h2>
        <div class="content has-text-justified">
          <img src="./static/images/atena_quanti_result.png" alt="Paper pipeline." style="width: 100%; padding-top: 10px"/>
          <p style="padding-top: 10px">
            <b>Experimental result on REVERIE dataset.</b> We evaluate ATENA on the REVERIE dataset by adapting three baseline models and comparing it with other test-time adaptation (TTA) methods.
            ATENA shows strong generalization to unseen environments, maintaining stable and reliable performance across both validation and test splits.
          </p>
          <!-- <br> -->
        </div>
      </div>
    </div>
  </div>
</section>

<hr>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Qualitative Results</h2>
        <b style="font-size: 1.1em">Before ATENA Adaptation</b>
          <div style="display: flex; flex-wrap: wrap; gap: 8px; justify-content: center; padding-top: 15px;">
            <!-- 1 row -->
            <video controls playsinline width="240">
              <source src="./static/videos/reverie_egocentric_307_duet.mp4" type="video/mp4">
            </video>
            <video controls playsinline width="240">
              <source src="./static/videos/reverie_egocentric_552_duet.mp4" type="video/mp4">
            </video>
            <video controls playsinline width="240">
              <source src="./static/videos/reverie_egocentric_3143_vs_duet.mp4" type="video/mp4">
            </video>
          </div>
          <br>
          <b style="font-size: 1.1em">After ATENA Adaptation</b>
          <div style="display: flex; flex-wrap: wrap; gap: 8px; justify-content: center; padding-top: 15px;">
            <!-- 2 row -->
            <video controls playsinline width="240">
              <source src="./static/videos/reverie_egocentric_307_atena.mp4" type="video/mp4">
            </video>
            <video controls playsinline width="240">
              <source src="./static/videos/reverie_egocentric_552_atena.mp4" type="video/mp4">
            </video>
            <video controls playsinline width="240">
              <source src="./static/videos/reverie_egocentric_3143_vs_atena.mp4" type="video/mp4">
            </video>
          </div>
          <br>
          <b style="font-size: 1.1em">Top-Down Visualization</b>
          <div style="display: flex; flex-wrap: wrap; gap: 8px; justify-content: center; padding-top: 15px;">
            <!-- 2 row -->
            <video controls playsinline width="240">
              <source src="./static/videos/trajectory_307_336_0.mp4" type="video/mp4">
            </video>
            <video controls playsinline width="240">
              <source src="./static/videos/trajectory_552_550_1.mp4" type="video/mp4">
            </video>
            <video controls playsinline width="240">
              <source src="./static/videos/trajectory_3143_125_0.mp4" type="video/mp4">
            </video>
          </div>
        <p style="padding-top: 10px">
          Navigation visualizations comparing the baseline VLN-DUET (Before ATENA Adaptation) and ours (After ATENA Adaptation).
        </p>
        <br>
        <div style="padding-top: 30px;">
          <img src="./static/images/atena_vis.png" alt="Navigation visualization of ATENA adaptation" style="width: 100%; padding-top: 10px;">
        </div>

        <p style="text-align: justify; padding-top: 15px;">
          This navigation visualization illustrates the changes in trajectory and action probabilities after ATENA adaptation.
          In the final step, ATENA successfully identifies the target object, demonstrating the effect of adaptation on navigation behavior.
          Moreover, you can see the top-down visualization showing trajectories before and after ATENA adaptation; additional examples are provided in the supplementary material.
        </p>
        <br>
      </div>
    </div>
  </div>
</section>



<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{ko2025active,
    title={Active Test-time Vision-Language Navigation},
    author={Ko, Heeju and Kim, Sungjune and Oh, Gyeongrok and Yoon, Jeongyoon and Lee, Honglak and Jang, Sujin and Kim, Seungryong and Kim, Sangpil},
    journal={arXiv preprint arXiv:2506.06630},
    year={2025}
    }</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
